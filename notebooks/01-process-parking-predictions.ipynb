{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime, timedelta, time\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "RAW_JSON_FILE = '/Users/adrianiraeguialvear/Desktop/OnSpotML_v2/data/raw/parking_predictions_corrected.json'\n",
    "OUTPUT_DIR = '../data/processed/'\n",
    "OUTPUT_FILENAME = 'parking_predictions_processed.parquet'\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_DIR, OUTPUT_FILENAME)\n",
    "\n",
    "# Prediction constants based on metadata\n",
    "PREDICTION_START_HOUR = 8\n",
    "PREDICTION_END_HOUR = 20\n",
    "MINUTES_PER_SLOT = 5\n",
    "SLOTS_PER_HOUR = 60 // MINUTES_PER_SLOT\n",
    "SLOTS_PER_DAY = 288 # Data provides 288 slots (24 hours)\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw JSON from: /Users/adrianiraeguialvear/Desktop/OnSpotML_v2/data/raw/parking_predictions_corrected.json\n",
      "Data appears nested under key: OPENDATA_PSIU_APPARKB\n",
      "Normalizing nested 'TRAMOS' data...\n",
      "Finished processing 1 date records.\n",
      "Concatenating 1 temporary DataFrames...\n",
      "Successfully loaded and normalized raw data. Shape: (6381, 9)\n",
      "Columns: ['FH_INICIO', 'type', 'geometry', 'ID_TRAMO', 'TRAMO', 'TIPO', 'TARIFA', 'HORARIO', 'PREDICCIONES']\n",
      "JSON loading and normalization completed in 0.12 seconds.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load Raw JSON Data (Memory Efficient & Handles FeatureCollection) ---\n",
    "print(f'Loading raw JSON from: {RAW_JSON_FILE}')\n",
    "start_load_time = time.time()\n",
    "\n",
    "# Using standard json.load()\n",
    "raw_data_list = None # Initialize\n",
    "df_raw = None        # Initialize final DataFrame\n",
    "processed_data_frames = [] # List to hold small DataFrames\n",
    "\n",
    "try:\n",
    "    with open(RAW_JSON_FILE, 'r') as f:\n",
    "        raw_data_full = json.load(f) # Load the entire JSON structure\n",
    "\n",
    "    # Check if data is nested under a key like 'OPENDATA_PSIU_APPARKB'\n",
    "    if isinstance(raw_data_full, dict) and len(raw_data_full) == 1:\n",
    "        key = list(raw_data_full.keys())[0]\n",
    "        print(f'Data appears nested under key: {key}')\n",
    "        raw_data_list = raw_data_full[key] # Get the list of date records\n",
    "    elif isinstance(raw_data_full, list):\n",
    "        print('Data appears as a list of date records.')\n",
    "        raw_data_list = raw_data_full\n",
    "    else:\n",
    "        raise ValueError(\"Loaded JSON is not a list or a single-key dictionary containing a list.\")\n",
    "\n",
    "    if not raw_data_list:\n",
    "         raise ValueError(\"The primary list of data records is empty.\")\n",
    "\n",
    "    # --- Normalize the nested structure (Processing in chunks/individually) ---\n",
    "    print(\"Normalizing nested 'TRAMOS' data...\")\n",
    "    records_to_process = raw_data_list # Process all records\n",
    "    num_processed = 0\n",
    "\n",
    "    for date_record in records_to_process:\n",
    "        fh_inicio = date_record.get('FH_INICIO')\n",
    "        tramos_outer_list = date_record.get('TRAMOS') # This list contains the FeatureCollection\n",
    "        current_date_features = [] # List to hold the actual features for this date\n",
    "\n",
    "        # Check if FH_INICIO is missing\n",
    "        if not fh_inicio:\n",
    "            print(f\"WARN: Skipping date record due to missing 'FH_INICIO': {date_record}\")\n",
    "            continue\n",
    "\n",
    "        # Ensure tramos_outer_list is a list, default to empty list if not\n",
    "        if not isinstance(tramos_outer_list, list):\n",
    "            print(f\"WARN: 'TRAMOS' for FH_INICIO {fh_inicio} is not a list, treating as empty: {tramos_outer_list}\")\n",
    "            tramos_outer_list = []\n",
    "\n",
    "        # Iterate through items in the outer TRAMOS list (expecting one FeatureCollection)\n",
    "        for item in tramos_outer_list:\n",
    "            if isinstance(item, dict) and item.get('type') == 'FeatureCollection':\n",
    "                # Found the FeatureCollection, get the actual features list\n",
    "                features_list = item.get('features')\n",
    "                if isinstance(features_list, list):\n",
    "                    # Process the actual features\n",
    "                    for feature_record in features_list:\n",
    "                        if isinstance(feature_record, dict):\n",
    "                            # Add the date to each feature's record (can add to properties or top-level)\n",
    "                            # Adding to top-level for simplicity here:\n",
    "                            feature_record['FH_INICIO'] = fh_inicio\n",
    "                            # Add the extracted feature to our list for this date\n",
    "                            current_date_features.append(feature_record)\n",
    "                        else:\n",
    "                            print(f\"WARN: Item inside 'features' list is not a dictionary: {feature_record}\")\n",
    "                else:\n",
    "                     print(f\"WARN: 'features' key inside FeatureCollection is not a list: {features_list}\")\n",
    "            else:\n",
    "                print(f\"WARN: Item in 'TRAMOS' list is not a FeatureCollection dictionary: {item}\")\n",
    "\n",
    "\n",
    "        # If features were found for this date, create a small DataFrame\n",
    "        if current_date_features:\n",
    "            # Extract properties and geometry if needed, or flatten later\n",
    "            # For now, create DataFrame from the list of feature dicts\n",
    "            df_temp = pd.DataFrame(current_date_features)\n",
    "            processed_data_frames.append(df_temp)\n",
    "\n",
    "        num_processed += 1\n",
    "\n",
    "    # --- End of loop ---\n",
    "    print(f\"Finished processing {num_processed} date records.\")\n",
    "\n",
    "    # Concatenate all the small DataFrames\n",
    "    if processed_data_frames:\n",
    "        print(f\"Concatenating {len(processed_data_frames)} temporary DataFrames...\")\n",
    "        # Need to handle potential differences in columns if structure varies\n",
    "        # Flatten the properties dictionary for easier access later\n",
    "        all_processed_rows = []\n",
    "        for df_temp in processed_data_frames:\n",
    "             for index, feature_row in df_temp.iterrows():\n",
    "                  row_data = {}\n",
    "                  # Copy top-level keys (like FH_INICIO, type, potentially geometry)\n",
    "                  for col in ['FH_INICIO', 'type', 'geometry']: # Add other top-level keys if they exist\n",
    "                      if col in feature_row:\n",
    "                           row_data[col] = feature_row[col]\n",
    "                  # Flatten the properties dictionary\n",
    "                  properties = feature_row.get('properties')\n",
    "                  if isinstance(properties, dict):\n",
    "                       for prop_key, prop_value in properties.items():\n",
    "                            row_data[prop_key] = prop_value # Add properties keys\n",
    "                  all_processed_rows.append(row_data)\n",
    "\n",
    "        if all_processed_rows:\n",
    "             df_raw = pd.DataFrame(all_processed_rows)\n",
    "             print(f'Successfully loaded and normalized raw data. Shape: {df_raw.shape}')\n",
    "             print(f'Columns: {df_raw.columns.tolist()}')\n",
    "        else:\n",
    "             print(\"WARNING: No rows extracted after attempting to flatten features.\")\n",
    "             df_raw = pd.DataFrame() # Empty DataFrame\n",
    "\n",
    "        del processed_data_frames, all_processed_rows # Free memory\n",
    "    else:\n",
    "        print(\"WARNING: No valid feature records found after normalization (processed_data_frames is empty).\")\n",
    "        df_raw = pd.DataFrame() # Empty DataFrame\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f'ERROR: Raw JSON file not found at {RAW_JSON_FILE}')\n",
    "    df_raw = None\n",
    "except MemoryError:\n",
    "    print(f'ERROR: MemoryError loading/processing JSON. File might be too large.')\n",
    "    df_raw = None\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f'ERROR: Invalid JSON format - {e}')\n",
    "    df_raw = None\n",
    "except Exception as e:\n",
    "    print(f'ERROR loading or normalizing JSON: {e}')\n",
    "    print(\"Check JSON structure, key names ('FH_INICIO', 'TRAMOS'), and content.\")\n",
    "    df_raw = None\n",
    "\n",
    "end_load_time = time.time()\n",
    "if df_raw is not None and not df_raw.empty:\n",
    "    print(f'JSON loading and normalization completed in {end_load_time - start_load_time:.2f} seconds.')\n",
    "elif df_raw is not None and df_raw.empty:\n",
    "     print(f'JSON loading completed, but resulted in an empty DataFrame. Time: {end_load_time - start_load_time:.2f} seconds.')\n",
    "else:\n",
    "    print('Failed to create df_raw from JSON.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse_predictions function defined (Corrected, but not used by Optimized Step 3).\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Define the Prediction Parsing Function (Corrected, but NOT used by Optimized Step 3)\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time # Make sure time is imported\n",
    "# import numpy as np # Uncomment if using np.nan for 'R' or unexpected values\n",
    "\n",
    "# Function to parse the prediction string for a single row\n",
    "def parse_predictions(row):\n",
    "    \"\"\"\n",
    "    Parses the 'PREDICCIONES' string for a row and generates multiple records,\n",
    "    one for each 5-minute interval prediction covering a full 24-hour period.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): A row from the predictions DataFrame.\n",
    "                         Expected columns: 'ID_TRAMO', 'FH_INICIO', 'PREDICCIONES'.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary represents a\n",
    "              single prediction interval record. Returns an empty list if\n",
    "              input validation fails or an error occurs during parsing.\n",
    "    \"\"\"\n",
    "    # --- Input Validation ---\n",
    "    required_cols = ['ID_TRAMO', 'FH_INICIO', 'PREDICCIONES']\n",
    "    if not all(col in row.index for col in required_cols):\n",
    "        # if row.name < 5: # Print only for first 5 rows to avoid clutter\n",
    "        #     print(f\"DBG: Missing required columns for row {row.name}. Available: {row.index.tolist()}\")\n",
    "        return []\n",
    "\n",
    "    # Check for NaN values in the required columns\n",
    "    if pd.isna(row['ID_TRAMO']) or pd.isna(row['FH_INICIO']) or pd.isna(row['PREDICCIONES']):\n",
    "        # if row.name < 5: # Print only for first 5 rows to avoid clutter\n",
    "        #      print(f\"DBG: NaN check failed for row {row.name}. Values: ID={row['ID_TRAMO']}, FH={row['FH_INICIO']}, PRED={row['PREDICCIONES']}\")\n",
    "        return []\n",
    "\n",
    "    # Validate FH_INICIO format and convert to datetime\n",
    "    try:\n",
    "        start_dt = pd.to_datetime(row['FH_INICIO'], dayfirst=True) # Handle DD/MM/YYYY\n",
    "    except ValueError:\n",
    "        # if row.name < 5: # Print only for first 5 rows\n",
    "        #      print(f\"DBG: Date parsing failed for row {row.name}. Value: '{row['FH_INICIO']}'\")\n",
    "        return []\n",
    "\n",
    "    # Validate prediction string type and length\n",
    "    predictions_str = row['PREDICCIONES']\n",
    "    if not isinstance(predictions_str, str):\n",
    "        # if row.name < 5: # Print only for first 5 rows\n",
    "        #      print(f\"DBG: PREDICCIONES not a string for row {row.name}. Type: {type(predictions_str)}\")\n",
    "        return []\n",
    "\n",
    "    # Use the constant defined in the config cell for expected length\n",
    "    expected_length = SLOTS_PER_DAY # Should be 288 now\n",
    "    if len(predictions_str) != expected_length:\n",
    "        # if row.name < 5: # Print only for first 5 rows\n",
    "        #      print(f\"DBG: PREDICCIONES length mismatch for row {row.name}. Expected: {expected_length}, Got: {len(predictions_str)}\")\n",
    "        return []\n",
    "    # --- End Input Validation ---\n",
    "\n",
    "    parsed_data = []\n",
    "    # --- Main Parsing Logic ---\n",
    "    for i in range(expected_length): # Loop 288 times\n",
    "        timestamp = start_dt + timedelta(minutes=i * MINUTES_PER_SLOT)\n",
    "        pred_val = predictions_str[i]\n",
    "\n",
    "        # Map prediction value ('0', '1', 'R') - Updated Mapping\n",
    "        if pred_val == '0':\n",
    "            mapped_pred_val = 0 # Assuming '0' means Libre/Free\n",
    "        elif pred_val == '1':\n",
    "            mapped_pred_val = 1 # Assuming '1' means Ocupado/Occupied\n",
    "        elif pred_val == 'R':\n",
    "            mapped_pred_val = -1 # Handling 'R' (Reserved/Restricted) as -1\n",
    "        else:\n",
    "            mapped_pred_val = -99 # Assigning a distinct code for unexpected values\n",
    "            # print(f\"WARN: Unexpected prediction character '{pred_val}' in row {row.name} at index {i}. Assigning -99.\")\n",
    "\n",
    "        parsed_data.append({\n",
    "            'ID_TRAMO': row['ID_TRAMO'],\n",
    "            'timestamp': timestamp,\n",
    "            'prediction': mapped_pred_val\n",
    "        })\n",
    "    # --- End Main Parsing Logic ---\n",
    "\n",
    "    return parsed_data\n",
    "\n",
    "print(\"parse_predictions function defined (Corrected, but not used by Optimized Step 3).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nStarting optimized processing for 6381 raw records...\n",
      "Processing 6381 valid rows after initial filtering.\n",
      "\\nConverting data types...\n",
      "All prediction characters successfully mapped.\n",
      "Sorting data...\n",
      "\\nCreated processed DataFrame. Shape: (1837728, 3)\n",
      "\\nProcessed data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1837728 entries, 0 to 1837727\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Dtype         \n",
      "---  ------      -----         \n",
      " 0   ID_TRAMO    Int64         \n",
      " 1   timestamp   datetime64[ns]\n",
      " 2   prediction  Int8          \n",
      "dtypes: Int64(1), Int8(1), datetime64[ns](1)\n",
      "memory usage: 33.3 MB\n",
      "\\nProcessed data head:\n",
      "   ID_TRAMO           timestamp  prediction\n",
      "0         7 2024-09-17 08:00:00           0\n",
      "1         7 2024-09-17 08:05:00           0\n",
      "2         7 2024-09-17 08:10:00           0\n",
      "3         7 2024-09-17 08:15:00           0\n",
      "4         7 2024-09-17 08:20:00           0\n",
      "\\nProcessed data tail:\n",
      "         ID_TRAMO           timestamp  prediction\n",
      "1837723     17056 2024-09-18 07:35:00           0\n",
      "1837724     17056 2024-09-18 07:40:00           0\n",
      "1837725     17056 2024-09-18 07:45:00           0\n",
      "1837726     17056 2024-09-18 07:50:00           0\n",
      "1837727     17056 2024-09-18 07:55:00           0\n",
      "\\nPrediction code distribution:\n",
      "prediction\n",
      "0    0.980567\n",
      "1    0.001149\n",
      "2    0.000818\n",
      "3    0.017466\n",
      "Name: proportion, dtype: Float64\n",
      "Optimized data parsing and processing completed in 0.53 seconds.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Apply Parsing Function & Process Results (Optimized Version) ---\n",
    "import numpy as np # Need numpy\n",
    "\n",
    "df_processed = None # Initialize\n",
    "if df_raw is not None and not df_raw.empty:\n",
    "    print(f'\\\\nStarting optimized processing for {len(df_raw)} raw records...')\n",
    "    start_parse_time = time.time()\n",
    "\n",
    "    # --- 1. Prepare data for expansion ---\n",
    "    # Ensure correct data types for columns we need\n",
    "    df_raw['ID_TRAMO'] = pd.to_numeric(df_raw['ID_TRAMO'], errors='coerce')\n",
    "    df_raw['FH_INICIO'] = pd.to_datetime(df_raw['FH_INICIO'], dayfirst=True, errors='coerce')\n",
    "    # Keep only rows with valid ID and Start Date, and correct prediction length\n",
    "    expected_length = SLOTS_PER_DAY # Should be 288\n",
    "    df_valid = df_raw.dropna(subset=['ID_TRAMO', 'FH_INICIO'])\n",
    "    df_valid = df_valid[df_valid['PREDICCIONES'].str.len() == expected_length].copy()\n",
    "    print(f'Processing {len(df_valid)} valid rows after initial filtering.')\n",
    "\n",
    "    if not df_valid.empty:\n",
    "        # --- 2. Create Timestamps ---\n",
    "        # Create a time delta range for one day (0 min, 5 min, ..., 1435 min for 288 slots)\n",
    "        time_deltas = pd.to_timedelta(np.arange(expected_length) * MINUTES_PER_SLOT, unit='m')\n",
    "\n",
    "        # --- 3. Expand DataFrame ---\n",
    "        # Repeat each row 'expected_length' (288) times\n",
    "        df_expanded = df_valid.loc[np.repeat(df_valid.index.values, expected_length)].copy()\n",
    "\n",
    "        # --- 4. Calculate Correct Timestamps ---\n",
    "        # Use the repeated FH_INICIO and add the corresponding time delta for each slot\n",
    "        # We need to tile the time_deltas to match the expanded DataFrame length\n",
    "        df_expanded['timestamp'] = df_expanded['FH_INICIO'] + np.tile(time_deltas, len(df_valid))\n",
    "\n",
    "        # --- 5. Expand and Map Predictions ---\n",
    "        # Flatten the prediction strings into a single series of characters\n",
    "        all_pred_chars = np.concatenate(df_valid['PREDICCIONES'].apply(list).values)\n",
    "        df_expanded['prediction_char'] = all_pred_chars\n",
    "\n",
    "        # ----- Updated Mapping -----\n",
    "        mapping = {'0': 0, '1': 1, '2': 2, '3': 3, 'R': -1}\n",
    "        # -------------------------\n",
    "\n",
    "        # Use .map, fill unknown with -99\n",
    "        df_expanded['prediction'] = df_expanded['prediction_char'].map(mapping).fillna(-99)\n",
    "\n",
    "        # --- 6. Final DataFrame Assembly ---\n",
    "        # Select and reorder columns\n",
    "        df_processed = df_expanded[['ID_TRAMO', 'timestamp', 'prediction']].copy()\n",
    "\n",
    "        # --- 7. Data Type Conversion and Cleanup ---\n",
    "        print('\\\\nConverting data types...')\n",
    "        df_processed['ID_TRAMO'] = df_processed['ID_TRAMO'].astype('Int64')\n",
    "        # Int8 is still suitable for range -1, 0, 1, 2, 3 (and -99)\n",
    "        df_processed['prediction'] = df_processed['prediction'].astype('Int8')\n",
    "\n",
    "        # Check if any unexpected characters resulted in -99\n",
    "        unknown_chars_count = (df_processed['prediction'] == -99).sum()\n",
    "        if unknown_chars_count > 0:\n",
    "            print(f\"WARN: Found {unknown_chars_count} instances of unexpected prediction characters (mapped to -99).\")\n",
    "            # You can uncomment this again if needed after this run\n",
    "            # print(\"Unique unexpected characters:\", df_expanded.loc[df_processed['prediction'] == -99, 'prediction_char'].unique())\n",
    "        else:\n",
    "             print(\"All prediction characters successfully mapped.\")\n",
    "\n",
    "\n",
    "        # Sort by ID and Timestamp\n",
    "        print('Sorting data...')\n",
    "        df_processed.sort_values(by=['ID_TRAMO', 'timestamp'], inplace=True)\n",
    "        df_processed.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        print(f'\\\\nCreated processed DataFrame. Shape: {df_processed.shape}')\n",
    "        print('\\\\nProcessed data info:')\n",
    "        df_processed.info()\n",
    "        print('\\\\nProcessed data head:')\n",
    "        print(df_processed.head())\n",
    "        print('\\\\nProcessed data tail:')\n",
    "        print(df_processed.tail())\n",
    "        print('\\\\nPrediction code distribution:')\n",
    "        # DropNA before value_counts if using Int64/Int8 which are nullable\n",
    "        print(df_processed['prediction'].value_counts(normalize=True, dropna=False).sort_index())\n",
    "\n",
    "    else:\n",
    "        print('WARNING: No valid data found after initial filtering. Check input file structure, column names, date formats, and prediction string lengths.')\n",
    "\n",
    "    end_parse_time = time.time()\n",
    "    print(f'Optimized data parsing and processing completed in {end_parse_time - start_parse_time:.2f} seconds.')\n",
    "\n",
    "else:\n",
    "    print('\\\\nSkipping parsing due to JSON loading failure or empty raw dataframe.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nSaving processed data (1837728 rows) to: ../data/processed/parking_predictions_processed.parquet\n",
      "Processed data saved successfully in 0.12 seconds.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Save Processed Data ---\n",
    "\n",
    "if df_processed is not None and not df_processed.empty:\n",
    "    print(f'\\\\nSaving processed data ({len(df_processed)} rows) to: {OUTPUT_FILE}')\n",
    "    start_save_time = time.time()\n",
    "    try:\n",
    "        df_processed.to_parquet(OUTPUT_FILE, index=False, engine='pyarrow', compression='snappy')\n",
    "        end_save_time = time.time()\n",
    "        print(f'Processed data saved successfully in {end_save_time - start_save_time:.2f} seconds.')\n",
    "    except Exception as e:\n",
    "        print(f'ERROR saving processed data to Parquet: {e}')\n",
    "else:\n",
    "    print('\\\\nNo processed data available to save.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load Parquet file: ../data/processed/parking_predictions_processed.parquet\n",
      "\n",
      "File loaded successfully.\n",
      "Shape: (1837728, 3)\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1837728 entries, 0 to 1837727\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Dtype         \n",
      "---  ------      -----         \n",
      " 0   ID_TRAMO    Int64         \n",
      " 1   timestamp   datetime64[ns]\n",
      " 2   prediction  Int8          \n",
      "dtypes: Int64(1), Int8(1), datetime64[ns](1)\n",
      "memory usage: 33.3 MB\n",
      "\n",
      "Data Types:\n",
      "ID_TRAMO               Int64\n",
      "timestamp     datetime64[ns]\n",
      "prediction              Int8\n",
      "dtype: object\n",
      "\n",
      "Head:\n",
      "   ID_TRAMO           timestamp  prediction\n",
      "0         7 2024-09-17 08:00:00           0\n",
      "1         7 2024-09-17 08:05:00           0\n",
      "2         7 2024-09-17 08:10:00           0\n",
      "3         7 2024-09-17 08:15:00           0\n",
      "4         7 2024-09-17 08:20:00           0\n",
      "\n",
      "Tail:\n",
      "         ID_TRAMO           timestamp  prediction\n",
      "1837723     17056 2024-09-18 07:35:00           0\n",
      "1837724     17056 2024-09-18 07:40:00           0\n",
      "1837725     17056 2024-09-18 07:45:00           0\n",
      "1837726     17056 2024-09-18 07:50:00           0\n",
      "1837727     17056 2024-09-18 07:55:00           0\n",
      "\n",
      "Prediction code distribution:\n",
      "prediction\n",
      "0    0.980567\n",
      "1    0.001149\n",
      "2    0.000818\n",
      "3    0.017466\n",
      "Name: proportion, dtype: Float64\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Load and Verify Saved Parquet File ---\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Use the OUTPUT_FILE variable defined in the configuration cell\n",
    "parquet_file_path = OUTPUT_FILE\n",
    "\n",
    "print(f\"Attempting to load Parquet file: {parquet_file_path}\")\n",
    "\n",
    "if os.path.exists(parquet_file_path):\n",
    "    try:\n",
    "        df_loaded = pd.read_parquet(parquet_file_path)\n",
    "        print(\"\\nFile loaded successfully.\")\n",
    "        print(f\"Shape: {df_loaded.shape}\")\n",
    "\n",
    "        print(\"\\nInfo:\")\n",
    "        df_loaded.info()\n",
    "\n",
    "        print(\"\\nData Types:\")\n",
    "        print(df_loaded.dtypes)\n",
    "\n",
    "        print(\"\\nHead:\")\n",
    "        print(df_loaded.head())\n",
    "\n",
    "        print(\"\\nTail:\")\n",
    "        print(df_loaded.tail())\n",
    "\n",
    "        print(\"\\nPrediction code distribution:\")\n",
    "        print(df_loaded['prediction'].value_counts(normalize=True, dropna=False).sort_index())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR loading or verifying Parquet file: {e}\")\n",
    "else:\n",
    "    print(f\"\\nERROR: File not found at {parquet_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Analyzing Data Scope ---\n",
      "Timestamp range: 2024-09-17 08:00:00 to 2024-09-18 07:55:00\n",
      "Total time span covered: 0 days 23:55:00\n",
      "Rough number of days covered: 2\n",
      "\n",
      "Total unique ID_TRAMOs: 6381\n",
      "\n",
      "Checking record count per ID_TRAMO (expected: 288)...\n",
      "All ID_TRAMOs have the expected number of records.\n",
      "\n",
      "--- Analysis Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Confirm Data Scope and Completeness ---\n",
    "import pandas as pd\n",
    "\n",
    "print(\"--- Analyzing Data Scope ---\")\n",
    "\n",
    "# 1. Verify Date Range\n",
    "min_ts = df_loaded['timestamp'].min()\n",
    "max_ts = df_loaded['timestamp'].max()\n",
    "print(f\"Timestamp range: {min_ts} to {max_ts}\")\n",
    "\n",
    "# Calculate duration and expected slots\n",
    "time_span = max_ts - min_ts\n",
    "# Add one slot duration to max_ts to get the end of the last interval for calculation\n",
    "total_duration_minutes = (max_ts - min_ts + pd.Timedelta(minutes=MINUTES_PER_SLOT)).total_seconds() / 60\n",
    "expected_slots_total = total_duration_minutes / MINUTES_PER_SLOT\n",
    "print(f\"Total time span covered: {time_span}\")\n",
    "# Note: Expected slots might be slightly complex if start/end times aren't perfectly aligned with day boundaries.\n",
    "# Let's calculate expected slots per day more directly if possible\n",
    "# Assuming 288 slots per full day were processed based on previous steps\n",
    "num_days_rough = (max_ts.normalize() - min_ts.normalize()).days + 1\n",
    "print(f\"Rough number of days covered: {num_days_rough}\")\n",
    "# The exact expectation depends on whether the start/end times cross midnight\n",
    "# Given the head/tail, it looks like 1 full day (Sept 17 08:00 to Sept 18 07:55)\n",
    "# This means exactly 288 slots were generated per ID_TRAMO during processing.\n",
    "expected_slots_per_tramo = SLOTS_PER_DAY # Should be 288 from config\n",
    "\n",
    "# 2. Count Unique ID_TRAMOs\n",
    "unique_tramos = df_loaded['ID_TRAMO'].nunique()\n",
    "print(f\"\\nTotal unique ID_TRAMOs: {unique_tramos}\")\n",
    "\n",
    "# 3. Check Completeness per ID_TRAMO\n",
    "print(f\"\\nChecking record count per ID_TRAMO (expected: {expected_slots_per_tramo})...\")\n",
    "record_counts = df_loaded.groupby('ID_TRAMO').size()\n",
    "\n",
    "# Find ID_TRAMOs with counts different from the expected value\n",
    "incomplete_tramos = record_counts[record_counts != expected_slots_per_tramo]\n",
    "\n",
    "if incomplete_tramos.empty:\n",
    "    print(\"All ID_TRAMOs have the expected number of records.\")\n",
    "else:\n",
    "    print(f\"Found {len(incomplete_tramos)} ID_TRAMOs with incomplete/unexpected record counts:\")\n",
    "    print(incomplete_tramos)\n",
    "\n",
    "print(\"\\n--- Analysis Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
