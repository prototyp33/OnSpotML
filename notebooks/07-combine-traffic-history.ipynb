{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Combine and Process Historical Traffic Status Data\n",
    "\n",
    "This notebook loads the monthly historical traffic CSV files downloaded previously,\n",
    "processes them (datetime conversion, resampling to 5-min intervals with \n",
    "forward-fill), combines them into a single DataFrame, and saves the result.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import time \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "# Adjust DATA_DIR if your notebook structure is different\n",
    "RAW_TRAFFIC_DIR = \"../data/raw/traffic_history\" \n",
    "PROCESSED_DIR = \"../data/processed\"\n",
    "OUTPUT_FILENAME = \"traffic_history_2022_2023_processed.parquet\" # Using Parquet for efficiency\n",
    "OUTPUT_FILE = os.path.join(PROCESSED_DIR, OUTPUT_FILENAME)\n",
    "\n",
    "# Define the date range we want to process (inclusive)\n",
    "START_YEAR, START_MONTH = 2022, 1\n",
    "END_YEAR, END_MONTH = 2023, 12\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for relevant CSV files in: ../data/raw/traffic_history\n",
      "Found 23 files to process for the period 2022-01 to 2023-12.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Find Relevant CSV Files ---\n",
    "print(f\"Scanning for relevant CSV files in: {RAW_TRAFFIC_DIR}\")\n",
    "all_csv_files = glob.glob(os.path.join(RAW_TRAFFIC_DIR, \"*.csv\"))\n",
    "\n",
    "files_to_process = []\n",
    "hist_pattern = re.compile(r\"^(\\d{4})_(\\d{2})_.*?_TRAMS_TRAMS\\.csv$\", re.IGNORECASE)\n",
    "\n",
    "for file_path in all_csv_files:\n",
    "    filename = os.path.basename(file_path)\n",
    "    match = hist_pattern.match(filename)\n",
    "    if match:\n",
    "        year = int(match.group(1))\n",
    "        month = int(match.group(2))\n",
    "        # Filter for the specific date range\n",
    "        if (year > START_YEAR or (year == START_YEAR and month >= START_MONTH)) and \\\n",
    "           (year < END_YEAR or (year == END_YEAR and month <= END_MONTH)):\n",
    "            files_to_process.append(file_path)\n",
    "\n",
    "# Sort files chronologically to process in order\n",
    "files_to_process.sort() \n",
    "\n",
    "print(f\"Found {len(files_to_process)} files to process for the period {START_YEAR}-{START_MONTH:02d} to {END_YEAR}-{END_MONTH:02d}.\")\n",
    "if not files_to_process:\n",
    "    print(\"Error: No files found for the specified date range. Please check the directory and date range.\")\n",
    "    # Stop execution if no files found\n",
    "    # exit() # Use exit() in a script, or just let the notebook stop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting processing loop...\n",
      "Processing file 1/23: 2022_01_Gener_TRAMS_TRAMS.csv...\n",
      "  Resampling 2022_01_Gener_TRAMS_TRAMS.csv to 5-minute intervals (this may take time)...\n",
      "  Resampling done in 0.27 seconds.\n",
      "  Finished processing 2022_01_Gener_TRAMS_TRAMS.csv. Rows added: 4705056\n",
      "Processing file 2/23: 2022_02_Febrer_TRAMS_TRAMS.csv...\n",
      "  Resampling 2022_02_Febrer_TRAMS_TRAMS.csv to 5-minute intervals (this may take time)...\n",
      "  Resampling done in 0.24 seconds.\n",
      "  Finished processing 2022_02_Febrer_TRAMS_TRAMS.csv. Rows added: 4249728\n",
      "Processing file 3/23: 2022_03_Marc_TRAMS_TRAMS.csv...\n",
      "  Resampling 2022_03_Marc_TRAMS_TRAMS.csv to 5-minute intervals (this may take time)...\n",
      "  Resampling done in 0.27 seconds.\n",
      "  Finished processing 2022_03_Marc_TRAMS_TRAMS.csv. Rows added: 4705056\n",
      "Processing file 4/23: 2022_04_Abril_TRAMS_TRAMS.csv...\n",
      "  Resampling 2022_04_Abril_TRAMS_TRAMS.csv to 5-minute intervals (this may take time)...\n",
      "  Resampling done in 0.31 seconds.\n",
      "  Finished processing 2022_04_Abril_TRAMS_TRAMS.csv. Rows added: 4595885\n",
      "Processing file 5/23: 2022_05_Maig_TRAMS_TRAMS.csv...\n",
      "  Resampling 2022_05_Maig_TRAMS_TRAMS.csv to 5-minute intervals (this may take time)...\n",
      "  Resampling done in 0.31 seconds.\n",
      "  Finished processing 2022_05_Maig_TRAMS_TRAMS.csv. Rows added: 4749696\n",
      "Processing file 6/23: 2022_06_Juny_TRAMS_TRAMS.csv...\n",
      "  Resampling 2022_06_Juny_TRAMS_TRAMS.csv to 5-minute intervals (this may take time)...\n",
      "  Resampling done in 0.28 seconds.\n",
      "  Finished processing 2022_06_Juny_TRAMS_TRAMS.csv. Rows added: 4596480\n",
      "Processing file 7/23: 2022_07_Juliol_TRAMS_TRAMS.csv...\n",
      "  Resampling 2022_07_Juliol_TRAMS_TRAMS.csv to 5-minute intervals (this may take time)...\n",
      "  Resampling done in 0.27 seconds.\n",
      "  Finished processing 2022_07_Juliol_TRAMS_TRAMS.csv. Rows added: 4749696\n",
      "Processing file 8/23: 2022_08_Agost_TRAMS_TRAMS.csv...\n",
      "  Resampling 2022_08_Agost_TRAMS_TRAMS.csv to 5-minute intervals (this may take time)...\n",
      "  Resampling done in 0.28 seconds.\n",
      "  Finished processing 2022_08_Agost_TRAMS_TRAMS.csv. Rows added: 4749696\n",
      "Processing file 9/23: 2022_09_Setembre_TRAMS_TRAMS.csv...\n",
      "  Resampling 2022_09_Setembre_TRAMS_TRAMS.csv to 5-minute intervals (this may take time)...\n",
      "  Resampling done in 0.29 seconds.\n",
      "  Finished processing 2022_09_Setembre_TRAMS_TRAMS.csv. Rows added: 4596480\n",
      "Processing file 10/23: 2022_10_Octubre_TRAMS_TRAMS.csv...\n",
      "  Resampling 2022_10_Octubre_TRAMS_TRAMS.csv to 5-minute intervals (this may take time)...\n",
      "  Resampling done in 0.28 seconds.\n",
      "  Finished processing 2022_10_Octubre_TRAMS_TRAMS.csv. Rows added: 4749696\n",
      "Processing file 11/23: 2022_11_Novembre_TRAMS_TRAMS.csv...\n",
      "  Resampling 2022_11_Novembre_TRAMS_TRAMS.csv to 5-minute intervals (this may take time)...\n",
      "  Resampling done in 0.28 seconds.\n",
      "  Finished processing 2022_11_Novembre_TRAMS_TRAMS.csv. Rows added: 4596480\n",
      "Processing file 12/23: 2022_12_Desembre_TRAMS_TRAMS.csv...\n",
      "  Resampling 2022_12_Desembre_TRAMS_TRAMS.csv to 5-minute intervals (this may take time)...\n",
      "  Resampling done in 0.28 seconds.\n",
      "  Finished processing 2022_12_Desembre_TRAMS_TRAMS.csv. Rows added: 4749696\n",
      "Processing file 13/23: 2023_01_Gener_TRAMS_TRAMS.csv...\n",
      "  Resampling 2023_01_Gener_TRAMS_TRAMS.csv to 5-minute intervals (this may take time)...\n",
      "  Resampling done in 0.28 seconds.\n",
      "  Finished processing 2023_01_Gener_TRAMS_TRAMS.csv. Rows added: 4749696\n",
      "Processing file 14/23: 2023_02_Febrer_TRAMS_TRAMS.csv...\n",
      "  Resampling 2023_02_Febrer_TRAMS_TRAMS.csv to 5-minute intervals (this may take time)...\n",
      "  Resampling done in 0.27 seconds.\n",
      "  Finished processing 2023_02_Febrer_TRAMS_TRAMS.csv. Rows added: 4290048\n",
      "Processing file 15/23: 2023_03_Marc_TRAMS_TRAMS.csv...\n",
      "  Resampling 2023_03_Marc_TRAMS_TRAMS.csv to 5-minute intervals (this may take time)...\n",
      "  Resampling done in 0.27 seconds.\n",
      "  Finished processing 2023_03_Marc_TRAMS_TRAMS.csv. Rows added: 4749696\n",
      "Processing file 16/23: 2023_04_Abril_TRAMS_TRAMS.csv...\n",
      "  Resampling 2023_04_Abril_TRAMS_TRAMS.csv to 5-minute intervals (this may take time)...\n",
      "  Resampling done in 0.28 seconds.\n",
      "  Finished processing 2023_04_Abril_TRAMS_TRAMS.csv. Rows added: 4596480\n",
      "Processing file 17/23: 2023_05_Maig_TRAMS_TRAMS.csv...\n",
      "  Resampling 2023_05_Maig_TRAMS_TRAMS.csv to 5-minute intervals (this may take time)...\n",
      "  Resampling done in 0.26 seconds.\n",
      "  Finished processing 2023_05_Maig_TRAMS_TRAMS.csv. Rows added: 4749696\n",
      "Processing file 18/23: 2023_06_Juny_TRAMS_TRAMS.csv...\n",
      "  Resampling 2023_06_Juny_TRAMS_TRAMS.csv to 5-minute intervals (this may take time)...\n",
      "  Resampling done in 0.23 seconds.\n",
      "  Finished processing 2023_06_Juny_TRAMS_TRAMS.csv. Rows added: 4595416\n",
      "Processing file 19/23: 2023_07_Juliol_TRAMS_TRAMS.csv...\n",
      "  Resampling 2023_07_Juliol_TRAMS_TRAMS.csv to 5-minute intervals (this may take time)...\n",
      "  Resampling done in 0.24 seconds.\n",
      "  Finished processing 2023_07_Juliol_TRAMS_TRAMS.csv. Rows added: 4749696\n",
      "Processing file 20/23: 2023_08_Agost_TRAMS_TRAMS.csv...\n",
      "  Resampling 2023_08_Agost_TRAMS_TRAMS.csv to 5-minute intervals (this may take time)...\n",
      "  Resampling done in 0.23 seconds.\n",
      "  Finished processing 2023_08_Agost_TRAMS_TRAMS.csv. Rows added: 4749696\n",
      "Processing file 21/23: 2023_09_Setembre_TRAMS_TRAMS.csv...\n",
      "  Dropped 1064 duplicate timestamp entries for specific TRAMs.\n",
      "  Resampling 2023_09_Setembre_TRAMS_TRAMS.csv to 5-minute intervals (this may take time)...\n",
      "  Resampling done in 0.22 seconds.\n",
      "  Finished processing 2023_09_Setembre_TRAMS_TRAMS.csv. Rows added: 4595416\n",
      "Processing file 22/23: 2023_11_Novembre_TRAMS_TRAMS.csv...\n",
      "  Resampling 2023_11_Novembre_TRAMS_TRAMS.csv to 5-minute intervals (this may take time)...\n",
      "  Resampling done in 0.23 seconds.\n",
      "  Finished processing 2023_11_Novembre_TRAMS_TRAMS.csv. Rows added: 4595948\n",
      "Processing file 23/23: 2023_12_Desembre_TRAMS_TRAMS.csv...\n",
      "  Resampling 2023_12_Desembre_TRAMS_TRAMS.csv to 5-minute intervals (this may take time)...\n",
      "  Resampling done in 0.24 seconds.\n",
      "  Finished processing 2023_12_Desembre_TRAMS_TRAMS.csv. Rows added: 4749164\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Process Files and Combine ---\n",
    "processed_dfs = []\n",
    "total_rows_processed = 0\n",
    "expected_columns = ['ID_TRAM', 'DataHoraLectura', 'EstatActual', 'PrevisioActual']\n",
    "\n",
    "print(\"\\nStarting processing loop...\")\n",
    "start_time_loop = time.time()\n",
    "\n",
    "for i, file_path in enumerate(files_to_process):\n",
    "    filename = os.path.basename(file_path)\n",
    "    print(f\"Processing file {i+1}/{len(files_to_process)}: {filename}...\")\n",
    "    try:\n",
    "        # Load CSV\n",
    "        df_month = pd.read_csv(file_path, sep=',', header=0, on_bad_lines='warn')\n",
    "        \n",
    "        # Basic validation and renaming\n",
    "        rename_map = {\n",
    "             'idTram': 'ID_TRAM', \n",
    "             'data': 'DataHoraLectura', \n",
    "             'estatActual': 'EstatActual', \n",
    "             'estatPrevist': 'PrevisioActual' \n",
    "        }\n",
    "        # Check if expected columns exist before renaming\n",
    "        missing_cols = [col for col in rename_map.keys() if col not in df_month.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"  Warning: Missing expected columns {missing_cols} in {filename}. Skipping file.\")\n",
    "            continue # Skip this file if columns are missing\n",
    "            \n",
    "        df_month.rename(columns=rename_map, inplace=True)\n",
    "        \n",
    "        # Select only necessary columns early to save memory\n",
    "        df_month = df_month[['ID_TRAM', 'DataHoraLectura', 'EstatActual', 'PrevisioActual']]\n",
    "\n",
    "        # Convert DataHoraLectura to datetime\n",
    "        df_month['Timestamp'] = pd.to_datetime(df_month['DataHoraLectura'], format='%Y%m%d%H%M%S', errors='coerce')\n",
    "        df_month.dropna(subset=['Timestamp'], inplace=True) # Drop rows with invalid dates\n",
    "        \n",
    "        if df_month.empty:\n",
    "             print(f\"  Warning: No valid data after datetime conversion in {filename}. Skipping.\")\n",
    "             continue\n",
    "\n",
    "        # --- Handle Duplicate Timestamps before setting index ---\n",
    "        # Sort first to ensure 'last' is deterministic if needed\n",
    "        df_month.sort_values(by=['ID_TRAM', 'Timestamp'], inplace=True)\n",
    "        # Keep the last entry for any ID_TRAM/Timestamp duplicates\n",
    "        rows_before_dedup = len(df_month)\n",
    "        df_month.drop_duplicates(subset=['ID_TRAM', 'Timestamp'], keep='last', inplace=True)\n",
    "        rows_after_dedup = len(df_month)\n",
    "        if rows_before_dedup > rows_after_dedup:\n",
    "             print(f\"  Dropped {rows_before_dedup - rows_after_dedup} duplicate timestamp entries for specific TRAMs.\")\n",
    "\n",
    "        # --- Resampling and Forward Filling ---\n",
    "        # This is memory intensive!\n",
    "        print(f\"  Resampling {filename} to 5-minute intervals (this may take time)...\")\n",
    "        start_resample_time = time.time()\n",
    "        \n",
    "        df_month.set_index('Timestamp', inplace=True)\n",
    "        # Group by tram, then resample each group and forward fill\n",
    "        # Selecting columns inside apply for potential memory saving\n",
    "        df_resampled = df_month.groupby('ID_TRAM', group_keys=True).apply(\n",
    "            lambda g: g[['EstatActual', 'PrevisioActual']].resample('5min').ffill(),\n",
    "            include_groups=False # Avoid adding ID_TRAMlevel to MultiIndex if pandas version supports it\n",
    "        )\n",
    "\n",
    "        # Reset index to bring 'ID_TRAM' and 'Timestamp' back as columns\n",
    "        # Check if ID_TRAM is in index levels after apply\n",
    "        if 'ID_TRAM' in df_resampled.index.names:\n",
    "            df_resampled.reset_index(inplace=True)\n",
    "        else: \n",
    "            # If group_keys=False worked or pandas version differs\n",
    "            df_resampled = df_resampled.reset_index()\n",
    "            # Need to manually add ID_TRAM back if it wasn't preserved - this is tricky!\n",
    "            # A safer approach might be to loop through trams if the above fails.\n",
    "            # For now, assume ID_TRAM is in the index or needs re-merging (less efficient)\n",
    "            print(f\"  Warning: Resampling might require adjustment if ID_TRAM is lost.\")\n",
    "\n",
    "\n",
    "        end_resample_time = time.time()\n",
    "        print(f\"  Resampling done in {end_resample_time - start_resample_time:.2f} seconds.\")\n",
    "\n",
    "        # Keep only essential columns after resampling\n",
    "        df_resampled = df_resampled[['ID_TRAM', 'Timestamp', 'EstatActual', 'PrevisioActual']]\n",
    "        \n",
    "        # Convert status columns to integer type (after ffill they might be float)\n",
    "        # Using nullable integer type Int8 for memory efficiency, allows NaN if ffill didn't cover start\n",
    "        df_resampled['EstatActual'] = df_resampled['EstatActual'].astype('Int8') \n",
    "        df_resampled['PrevisioActual'] = df_resampled['PrevisioActual'].astype('Int8')\n",
    "\n",
    "        processed_dfs.append(df_resampled)\n",
    "        total_rows_processed += len(df_resampled)\n",
    "        print(f\"  Finished processing {filename}. Rows added: {len(df_resampled)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR processing file {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concatenating all processed monthly DataFrames...\n",
      "Concatenation complete in 0.67 seconds.\n",
      "Total rows in combined dataset: 106964597\n",
      "\n",
      "Saving combined data to: ../data/processed/traffic_history_2022_2023_processed.parquet\n",
      "Saved successfully in 4.42 seconds.\n",
      "\n",
      "--- Info of Final Combined DataFrame ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 106964597 entries, 0 to 106964596\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Dtype         \n",
      "---  ------          -----         \n",
      " 0   ID_TRAM         int64         \n",
      " 1   Timestamp       datetime64[ns]\n",
      " 2   EstatActual     Int8          \n",
      " 3   PrevisioActual  Int8          \n",
      "dtypes: Int8(2), datetime64[ns](1), int64(1)\n",
      "memory usage: 2.0 GB\n",
      "\n",
      "Total processing time: 0.56 minutes\n",
      "\n",
      "--- End of Notebook ---\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Concatenate All Processed DataFrames ---\n",
    "if processed_dfs:\n",
    "    print(\"\\nConcatenating all processed monthly DataFrames...\")\n",
    "    start_concat_time = time.time()\n",
    "    combined_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "    end_concat_time = time.time()\n",
    "    print(f\"Concatenation complete in {end_concat_time - start_concat_time:.2f} seconds.\")\n",
    "    print(f\"Total rows in combined dataset: {len(combined_df)}\")\n",
    "    \n",
    "    # --- 4. Save Combined Data ---\n",
    "    print(f\"\\nSaving combined data to: {OUTPUT_FILE}\")\n",
    "    try:\n",
    "        start_save_time = time.time()\n",
    "        combined_df.to_parquet(OUTPUT_FILE, index=False, engine='pyarrow') # Or 'fastparquet'\n",
    "        end_save_time = time.time()\n",
    "        print(f\"Saved successfully in {end_save_time - start_save_time:.2f} seconds.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR saving combined data: {e}\")\n",
    "        \n",
    "    # Optional: Display info of final DataFrame\n",
    "    print(\"\\n--- Info of Final Combined DataFrame ---\")\n",
    "    combined_df.info(memory_usage='deep')\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo dataframes were processed. Skipping concatenation and saving.\")\n",
    "\n",
    "end_time_loop = time.time()\n",
    "print(f\"\\nTotal processing time: {(end_time_loop - start_time_loop) / 60:.2f} minutes\")\n",
    "print(\"\\n--- End of Notebook ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
